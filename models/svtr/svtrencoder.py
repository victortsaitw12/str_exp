import torch
import torch.nn as nn
import numpy as np
from .overlabpatchembed import OverlapPatchEmbed
from .mixinblock import MixingBlock
from .merginblock import MerigingBlock
import math
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    def norm_cdf(x):
      # Computes standard normal cumulative distribution function
      return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print('mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [lower, upper], then translate
        # to [2lower-1, 2upper-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def trunc_normal_init(module, mean=0, std=1, a=-2, b=2, bias=0):
    if hasattr(module, 'weight') and module.weight is not None:
        trunc_normal_(module.weight, mean, std, a, b)  # type: ignore
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)  # type: ignore
    
class SVTREncoder(nn.Module):
    """A PyTorch implementation of `SVTR: Scene Text Recognition with a Single
    Visual Model <https://arxiv.org/abs/2205.00159>`_
    Code is partially modified from https://github.com/PaddlePaddle/PaddleOCR.
    Args:
        img_size (Tuple[int, int], optional): The expected input image shape.
            Defaults to [32, 100].
        in_channels (int, optional): The num of input channels. Defaults to 3.
        embed_dims (Tuple[int, int, int], optional): Number of input channels.
            Defaults to [64, 128, 256].
        depth (Tuple[int, int, int], optional):
            The number of MixingBlock at each stage. Defaults to [3, 6, 3].
        num_heads (Tuple[int, int, int], optional): Number of attention heads.
            Defaults to [2, 4, 8].
        mixer_types (Tuple[str], optional): Mixing type in a MixingBlock.
            Defaults to ['Local']*6+['Global']*6.
        window_size (Tuple[Tuple[int, int]], optional):
            The height and width of the window at eeach stage.
            Defaults to [[7, 11], [7, 11], [7, 11]].
        merging_types (str, optional): The way of downsample in MergingBlock.
            Defaults to 'Conv'.
        mlp_ratio (int, optional): Ratio of hidden features to input in MLP.
            Defaults to 4.
        qkv_bias (bool, optional):
            Whether to add bias for qkv in attention modules. Defaults to True.
        qk_scale (float, optional): A scaling factor. Defaults to None.
        drop_rate (float, optional): Probability of an element to be zeroed.
            Defaults to 0.0.
        last_drop (float, optional): cfg of dropout at last stage.
            Defaults to 0.1.
        attn_drop_rate (float, optional): _description_. Defaults to 0..
        drop_path_rate (float, optional): stochastic depth rate.
            Defaults to 0.1.
        out_channels (int, optional): The num of output channels in backone.
            Defaults to 192.
        max_seq_len (int, optional): Maximum output sequence length :math:`T`.
            Defaults to 25.
        num_layers (int, optional): The num of conv in PatchEmbedding.
            Defaults to 2.
        prenorm (bool, optional): Whether to place the MixingBlock before norm.
            Defaults to True.
        init_cfg (dict or list[dict], optional): Initialization configs.
            Defaults to None.
    """

    def __init__(self, img_size=[32, 100],
                 in_channels= 3,
                 embed_dims= [64, 128, 256],
                 depth= [3, 6, 3],
                 num_heads= [2, 4, 8],
                 mixer_types= ['Local'] * 6 + ['Global'] * 6,
                 window_size=[[7, 11], [7, 11], [7, 11]],
                 merging_types='Conv',
                 mlp_ratio=4,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 last_drop=0.1,
                 attn_drop_rate=0.,
                 drop_path_rate=0.1,
                 out_channels=192,
                 max_seq_len=25,
                 num_layers=2,
                 prenorm=True,
                 init_cfg=None):
        super(SVTREncoder, self).__init__()
        self.img_size = img_size
        self.embed_dims = embed_dims
        self.out_channels = out_channels
        self.prenorm = prenorm
        self.patch_embed = OverlapPatchEmbed(
            in_channels=in_channels,
            embed_dims=embed_dims[0],
            num_layers=num_layers)
        num_patches = (img_size[1] // (2**num_layers)) * (
            img_size[0] // (2**num_layers))
        self.input_shape = [
            img_size[0] // (2**num_layers), img_size[1] // (2**num_layers)
        ]
        print('input_shape:', self.input_shape)
        self.absolute_pos_embed = nn.Parameter(
            torch.zeros([1, num_patches, embed_dims[0]], dtype=torch.float32),
            requires_grad=True)
        self.pos_drop = nn.Dropout(drop_rate)
        dpr = np.linspace(0, drop_path_rate, sum(depth))

        self.blocks1 = nn.ModuleList([
            MixingBlock(
                embed_dims=embed_dims[0],
                num_heads=num_heads[0],
                mixer=mixer_types[0:depth[0]][i],
                window_size=window_size[0],
                input_shape=self.input_shape,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[0:depth[0]][i],
                prenorm=prenorm) for i in range(depth[0])
        ])
        self.downsample1 = MerigingBlock(
            in_channels=embed_dims[0],
            out_channels=embed_dims[1],
            types=merging_types,
            stride=[2, 1])
        input_shape = [self.input_shape[0] // 2, self.input_shape[1]]
        self.merging_types = merging_types

        self.blocks2 = nn.ModuleList([
            MixingBlock(
                embed_dims=embed_dims[1],
                num_heads=num_heads[1],
                mixer=mixer_types[depth[0]:depth[0] + depth[1]][i],
                window_size=window_size[1],
                input_shape=input_shape,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[depth[0]:depth[0] + depth[1]][i],
                prenorm=prenorm) for i in range(depth[1])
        ])
        self.downsample2 = MerigingBlock(
            in_channels=embed_dims[1],
            out_channels=embed_dims[2],
            types=merging_types,
            stride=[2, 1])
        input_shape = [self.input_shape[0] // 4, self.input_shape[1]]

        self.blocks3 = nn.ModuleList([
            MixingBlock(
                embed_dims=embed_dims[2],
                num_heads=num_heads[2],
                mixer=mixer_types[depth[0] + depth[1]:][i],
                window_size=window_size[2],
                input_shape=input_shape,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[depth[0] + depth[1]:][i],
                prenorm=prenorm) for i in range(depth[2])
        ])
        self.layer_norm = nn.LayerNorm(self.embed_dims[-1], eps=1e-6)
        self.avgpool = nn.AdaptiveAvgPool2d([1, max_seq_len])
        self.last_conv = nn.Conv2d(
            in_channels=embed_dims[2],
            out_channels=self.out_channels,
            kernel_size=1,
            bias=False,
            stride=1,
            padding=0)
        self.hardwish = nn.Hardswish()
        self.dropout = nn.Dropout(p=last_drop)

        trunc_normal_init(self.absolute_pos_embed, mean=0, std=0.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_init(m.weight, mean=0, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.zeros_(m.bias)
        if isinstance(m, nn.LayerNorm):
            nn.init.zeros_(m.bias)
            nn.init.ones_(m.weight)
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(
                m.weight, mode='fan_out', nonlinearity='relu')

    def forward_features(self, x):
        """Forward function except the last combing operation.
        Args:
            x (torch.Tensor): A Tensor of shape :math:`(N, H, W, C)`.
        Returns:
            torch.Tensor: A Tensor of shape :math:`(N, H/16, W/4, 256)`.
        """
        x = self.patch_embed(x)
        x = x + self.absolute_pos_embed
        x = self.pos_drop(x)
        for blk in self.blocks1:
            x = blk(x)
        x = self.downsample1(
            x.permute(0, 2, 1).reshape([
                -1, self.embed_dims[0], self.input_shape[0],
                self.input_shape[1]
            ]))

        for blk in self.blocks2:
            x = blk(x)
        x = self.downsample2(
            x.permute(0, 2, 1).reshape([
                -1, self.embed_dims[1], self.input_shape[0] // 2,
                self.input_shape[1]
            ]))

        for blk in self.blocks3:
            x = blk(x)
        if not self.prenorm:
            x = self.layer_norm(x)
        return x

    def forward(self, x, data_samples=None):
        """Forward function.
        Args:
            x (torch.Tensor): A Tensor of shape :math:`(N, H/16, W/4, 256)`.
            data_samples (list[TextRecogDataSample]): Batch of
                TextRecogDataSample. Defaults to None.
        Returns:
            torch.Tensor: A Tensor of shape :math:`(N, 1, W/4, 192)`.
        """
        x = self.forward_features(x)
        x = x.permute(0, 2, 1).reshape([
                -1, 
                self.embed_dims[2], 
                self.input_shape[0] // 4,
                self.input_shape[1]
            ])
        x = self.avgpool(x)
        x = self.last_conv(x)
        x = self.hardwish(x)
        x = self.dropout(x)

        x = x.squeeze(2)
        x = x.permute(0, 2, 1)
        return x